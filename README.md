# Chapter 1. Introduction to Retrieval Augmented Generation (RAG)

## How Does RAG Work?
<img width="750" height="300" alt="image" src="https://github.com/user-attachments/assets/43c72e05-b3d4-418a-8ac6-b38bbd5e0e82" />

RAG has two steps R -> Retrival & G -> Generation 
R -->  When a user issues a query to RAG system, R - step kicks in first to retrieve information that is most relevant to the question (or query).

G --> Then follows the “G” step, where a response is generated by tasking a large language model to analyze the retrieved information and the query, and craft a proper response to the query grounded in the facts retrieved.

A --> The word “augmented” in “Retrieval Augmented Generation” suggests that the retrieved information is added (that’s the meaning of the word “augment”) to the prompt of an LLM for generation. A RAG prompt typically looks something like this:

“””
Here is a user query: {query}.
And relevant context:
{context}
Please respond to the user query using the context
“””


In many ways, the difference between pure LLM use and RAG is similar to the difference between a closed-book test and an open-book test. In a closed-book test, students must rely solely on their memory and understanding. No textbooks, notes, or other reference materials are allowed. Similarly, pure LLM usage means that all the information you get is based solely on the dataset included during the LLM training. Such knowledge is stored in the parameters of an LLM, which is an artificial neural network whose behavior is determined by the values of its weights, and thus referred to as the parametric knowledge.

In contrast, in an open-book test, students can consult textbooks, notes, or other approved materials during the exam. This setup allows them to refer back to detailed information if needed, and is exactly how RAG works - the retrieval step provides additional information to the LLM in real time.

 Let’s now go a level deeper to understand what components are required for RAG and how the ingest and query flows work.
 
## The Blue print of RAG stack 

below depicts two flows: the ingest flow and the query flow
<img width="750" height="300" alt="image" src="https://github.com/user-attachments/assets/206db16f-eb55-495e-a34d-c9459712feea" />

The ingest flow performs those functions needed to extract the data from its source (like a database, a set of PDF files on S3, text on Notion, etc) and index it into the RAG stack.

The query flow performs the full processing of a user query - retrieves the right facts and uses the LLM for generative summary, resulting in a response to the end user.

Let’s look at these in more detail.

## The Ingest Flow

How Ingest Flow Works in RAG

1. Data comes in → All documents (text, etc.) are prepared for storage.
2. Convert to vectors (embeddings) → Each piece of text is transformed into a vector that captures its meaning, not just words.
3. Store in Vector DB → Both the vector and the original text are saved in a Vector Database.
4. This process = Indexing/Embedding → Different names, same idea: preparing data so queries can later match it.
5. More advanced RAG → Can also index tables, charts, images, or videos, not just plain text.
   
## The Query FLow
How Query Flow Works in RAG

1. User asks a question → The query is converted into a vector (embedding).

2. Search in Vector DB → The system finds similar facts from stored data using semantic search (not keyword-based, but meaning-based).

3. Relevant facts retrieved → The system pulls the most useful pieces of text related to the query.

#### Answer Generation
4. Prompt crafted → A special prompt is created for the LLM (Large Language Model), telling it to use the retrieved facts to answer.

5. LLM responds with answer → The response is grounded in the facts, often with citations/references to show sources
#### Quality check
6. Hallucination detection → Ensures the LLM didn’t make things up and is sticking to the retrieved facts.

7. Other guardrails → Check for bias, harmful, or disallowed content before showing the answer to the user.

## RAG vs. “chatting with PDF”

#### “Chat with PDF” (Simple Approach)
1. Takes the entire PDF text and puts it directly into the LLM prompt.
2. Works fine for small PDFs (LLMs can handle 128K–1M tokens now).
3. You can even fit tens or hundreds of PDFs into the context window.
But: this approach does not scale when you have hundreds of thousands of documents.

#### Why RAG is Better
1. Scalability → RAG retrieves only relevant chunks from massive document collections (Google Drive, Notion, SharePoint, S3, etc.).
2. Cost efficiency → Don’t waste money sending irrelevant text into the LLM.
3. Low latency → Faster because the LLM processes only small, relevant parts instead of huge documents.
4. Accuracy → Ensures answers are grounded in the right documents instead of relying on the user to decide what to feed in.

## RAG vs. fine-tuning
Fine-tuning (What it is)? 
1.Take a pre-trained LLM and train it further on your domain data.
Goal: The model learns patterns, terminology, and knowledge unique to your business.

#### Problems with Fine-tuning
1.Hard to do well → Needs deep ML expertise; risk of overfitting, bias, or breaking general language skills.
2. Data requirements → Needs large, clean datasets; many companies don’t have enough.
3.High cost → GPUs are expensive; frequent fine-tuning isn’t practical.
4.Not flexible → If data changes daily/weekly, you’d need to fine-tune repeatedly → not scalable.
5. Access control issues → All knowledge becomes one “blob.” No way to separate HR-only data from Finance-only data.
     Example: An employee could see confidential CEO-only info.
6. Multiple models needed → To fix access issues, you’d fine-tune separate models per department → complex + costly.

#### Why RAG is Better? 

1. Scalable → No need to retrain the LLM; just update the database with new documents.
2. Cost-effective → Retrieve only relevant facts instead of embedding everything into model weights.
3.Access control → Add permissions/filters at query time → only show what each user is allowed to see.
4.Flexible → Can work with mixed data sources (PDFs, databases, docs, etc.).
